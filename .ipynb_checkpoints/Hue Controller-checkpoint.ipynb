{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the Hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phue import Bridge\n",
    "\n",
    "# https://www.meethue.com/api/nupnp\n",
    "b = Bridge('192.168.1.90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'success': {'/lights/3/state/on': True}}]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Switch light 3 on\n",
    "b.set_light(3,'on', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prints if light 1 is on or not\n",
    "b.get_light(3, 'on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'success': {'/lights/3/state/bri': 254}}]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set brightness of lamp 3 to 100%\n",
    "b.set_light(3, 'bri', 254)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'success': {'/lights/3/state/on': False}}]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Switch light 3 off\n",
    "b.set_light(3,'on', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice first application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "from bs4 import BeautifulSoup\n",
    "import speech_recognition as sr_audio \n",
    "import os, pyttsx3, pygame, time\n",
    "import speech_recognition as sr_audio \n",
    "\n",
    "r = sr_audio.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_record(filename, duration, fs, channels):\n",
    "    print('recording')\n",
    "    myrecording = sd.rec(int(duration * fs), samplerate=fs, channels=channels) \n",
    "    sd.wait()\n",
    "    sf.write(filename, myrecording, fs)\n",
    "    print('done recording')\n",
    "    \n",
    "def sync_playback(filename):\n",
    "    # takes in a file and plays it back \n",
    "    pygame.mixer.init() \n",
    "    pygame.mixer.music.load(filename) \n",
    "    pygame.mixer.music.play()\n",
    "    \n",
    "def speak_text(text): \n",
    "    engine=pyttsx3.init() \n",
    "    engine.setProperty('voice', 'com.apple.speech.synthesis.voice.Alex')\n",
    "    engine.say(text) \n",
    "    engine.runAndWait()\n",
    "    \n",
    "def transcribe_audio_sphinx(filename):\n",
    "# transcribe the audio (note this is only done if a voice sample) r=sr_audio.Recognizer()\n",
    "    with sr_audio.AudioFile(filename) as source:\n",
    "        audio = r.record(source)\n",
    "        \n",
    "    text=r.recognize_sphinx(audio) \n",
    "    print('transcript: '+text)\n",
    "    return text\n",
    "    \n",
    "#speak_text('What do you want to search?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recording\n",
      "done recording\n",
      "transcript: the other and all\n"
     ]
    }
   ],
   "source": [
    "sync_record('response.wav',2,16000,1) \n",
    "transcript=transcribe_audio_sphinx('response.wav')\n",
    "\n",
    "if transcript.lower().find('hello') >= 0:\n",
    "    b.set_light(3,'on', True)\n",
    "    \n",
    "if transcript.lower().find('bye') >= 0:\n",
    "    b.set_light(3,'on', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if transcript.lower().find('hello') >= 0:\n",
    "    b.set_light(3,'on', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import speech_v1\n",
    "from google.cloud.speech_v1 import enums\n",
    "import io\n",
    "\n",
    "def sample_long_running_recognize(local_file_path):\n",
    "    \"\"\"\n",
    "    Transcribe a long audio file using asynchronous speech recognition\n",
    "    Args:\n",
    "      local_file_path Path to local audio file, e.g. /path/audio.wav\n",
    "    \"\"\"\n",
    "\n",
    "    client = speech_v1.SpeechClient()\n",
    "\n",
    "    # local_file_path = 'resources/brooklyn_bridge.raw'\n",
    "\n",
    "    # The language of the supplied audio\n",
    "    language_code = \"fr-FR\"\n",
    "\n",
    "    # Sample rate in Hertz of the audio data sent\n",
    "    sample_rate_hertz = 16000\n",
    "\n",
    "    # Encoding of audio data sent. This sample sets this explicitly.\n",
    "    # This field is optional for FLAC and WAV audio formats.\n",
    "    encoding = enums.RecognitionConfig.AudioEncoding.LINEAR16\n",
    "    config = {\n",
    "        \"language_code\": language_code,\n",
    "        \"sample_rate_hertz\": sample_rate_hertz,\n",
    "        \"encoding\": encoding,\n",
    "    }\n",
    "    with io.open(local_file_path, \"rb\") as f:\n",
    "        content = f.read()\n",
    "    audio = {\"content\": content}\n",
    "\n",
    "    operation = client.long_running_recognize(config, audio)\n",
    "\n",
    "    print(u\"Waiting for operation to complete...\")\n",
    "    response = operation.result()\n",
    "\n",
    "    for result in response.results:\n",
    "        # First alternative is the most probable result\n",
    "        alternative = result.alternatives[0]\n",
    "        print(u\"Transcript: {}\".format(alternative.transcript))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS or explicitly create credentials and re-run the application. For more information, please see https://cloud.google.com/docs/authentication/getting-started",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDefaultCredentialsError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-3ef8bb068c19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_long_running_recognize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'response.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-53-4624016fb224>\u001b[0m in \u001b[0;36msample_long_running_recognize\u001b[0;34m(local_file_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \"\"\"\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspeech_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpeechClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# local_file_path = 'resources/brooklyn_bridge.raw'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/google/cloud/speech_v1/gapic/speech_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, transport, channel, credentials, client_config, client_info, client_options)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.transport = speech_grpc_transport.SpeechGrpcTransport(\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0maddress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_endpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             )\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/google/cloud/speech_v1/gapic/transports/speech_grpc_transport.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, channel, credentials, address)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 options={\n\u001b[1;32m     66\u001b[0m                     \u001b[0;34m\"grpc.max_send_message_length\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                     \u001b[0;34m\"grpc.max_receive_message_length\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 }.items(),\n\u001b[1;32m     69\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/google/cloud/speech_v1/gapic/transports/speech_grpc_transport.py\u001b[0m in \u001b[0;36mcreate_channel\u001b[0;34m(cls, address, credentials, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \"\"\"\n\u001b[1;32m    103\u001b[0m         return google.api_core.grpc_helpers.create_channel(\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscopes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_OAUTH_SCOPES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         )\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36mcreate_channel\u001b[0;34m(target, credentials, scopes, ssl_credentials, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \"\"\"\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcredentials\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscopes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscopes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         credentials = google.auth.credentials.with_scopes_if_required(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/google/auth/_default.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(scopes, request)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meffective_project_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefaultCredentialsError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_HELP_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mDefaultCredentialsError\u001b[0m: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS or explicitly create credentials and re-run the application. For more information, please see https://cloud.google.com/docs/authentication/getting-started"
     ]
    }
   ],
   "source": [
    "sample_long_running_recognize('response.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# People detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('/usr/local/lib/python3.7/site-packages/cv2/data/haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9256c0693a54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Video'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(30, 30),\n",
    "        flags=cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        b.set_light(3,'on', False)\n",
    "    else:\n",
    "        b.set_light(3,'on', True)\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 3)\n",
    "        cv2.imshow('Video', frame)\n",
    "        \n",
    "    time.sleep(1)\n",
    "    \n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
